{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Legal Clause Similarity Detection\n",
        "## Deep Learning Assignment 2 - CS425\n",
        "\n",
        "**Student:** Syed Taha Hasan  \n",
        "**FastID:** i211767  \n",
        "**Date:** November 2025\n",
        "\n",
        "### Objective\n",
        "Develop NLP models to identify semantic similarity between legal clauses using baseline architectures (BiLSTM and ESIM - Enhanced Sequential Inference Model) without pre-trained transformers.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setup complete!\n",
            "PyTorch version: 2.7.0+cpu\n",
            "CUDA available: False\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(42)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "# Import custom modules\n",
        "from data_loader import LegalClauseDataLoader\n",
        "from text_preprocessor import TextPreprocessor\n",
        "from models import BiLSTMSimilarityModel, ESIMSimilarityModel\n",
        "from trainer import ModelTrainer, ClausePairDataset\n",
        "from evaluator import ModelEvaluator\n",
        "from visualization import TrainingVisualizer\n",
        "\n",
        "print(\"Setup complete!\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Loading and Exploration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading 395 CSV files...\n",
            "Loaded 395 categories\n",
            "Total clauses: 150881\n",
            "Categories with most clauses: [('time-of-essence', 630), ('time-of-the-essence', 620), ('capitalized-terms', 590), ('definitions-and-interpretation', 590), ('captions', 580)]\n",
            "\n",
            "Dataset Statistics:\n",
            "  num_categories: 395\n",
            "  total_clauses: 150881\n",
            "  avg_clauses_per_category: 381.9772151898734\n",
            "  min_clauses_per_category: 15\n",
            "  max_clauses_per_category: 630\n"
          ]
        }
      ],
      "source": [
        "# Initialize data loader\n",
        "data_loader = LegalClauseDataLoader(data_dir=\"archive (1)\")\n",
        "\n",
        "# Load all data\n",
        "clauses_by_category = data_loader.load_all_data()\n",
        "\n",
        "# Get statistics\n",
        "stats = data_loader.get_statistics()\n",
        "print(\"\\nDataset Statistics:\")\n",
        "for key, value in stats.items():\n",
        "    print(f\"  {key}: {value}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created 10000 pairs: 5000 positive, 5000 negative\n",
            "\n",
            "Total pairs created: 10000\n",
            "Similar pairs: 5000\n",
            "Dissimilar pairs: 5000\n"
          ]
        }
      ],
      "source": [
        "# Create similarity pairs\n",
        "NUM_PAIRS = 10000  # Adjust based on computational resources\n",
        "POSITIVE_RATIO = 0.5  # 50% similar, 50% dissimilar\n",
        "\n",
        "pairs, labels = data_loader.create_similarity_pairs(\n",
        "    num_pairs=NUM_PAIRS,\n",
        "    positive_ratio=POSITIVE_RATIO,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "print(f\"\\nTotal pairs created: {len(pairs)}\")\n",
        "print(f\"Similar pairs: {sum(labels)}\")\n",
        "print(f\"Dissimilar pairs: {len(labels) - sum(labels)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Text Preprocessing and Data Splitting\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Building vocabulary...\n",
            "Vocabulary size: 10000\n",
            "Most common words: [('the', 182334), ('or', 174451), ('of', 161900), ('any', 122302), ('in', 86170), ('to', 68240), ('and', 64064), ('its', 38740), ('by', 35922), ('company', 35099)]\n",
            "\n",
            "Vocabulary size: 10000\n",
            "Max sequence length: 200\n",
            "\n",
            "Training pairs: 7000\n",
            "Validation pairs: 1500\n",
            "Test pairs: 1500\n"
          ]
        }
      ],
      "source": [
        "# Initialize preprocessor\n",
        "MAX_VOCAB_SIZE = 10000\n",
        "MAX_SEQ_LENGTH = 200\n",
        "\n",
        "preprocessor = TextPreprocessor(\n",
        "    max_vocab_size=MAX_VOCAB_SIZE,\n",
        "    max_seq_length=MAX_SEQ_LENGTH\n",
        ")\n",
        "\n",
        "# Build vocabulary from all clause texts\n",
        "all_texts = [text for pair in pairs for text in pair]\n",
        "preprocessor.build_vocabulary(all_texts)\n",
        "\n",
        "print(f\"\\nVocabulary size: {preprocessor.vocab_size}\")\n",
        "print(f\"Max sequence length: {preprocessor.max_seq_length}\")\n",
        "\n",
        "# Split data: 70% train, 15% validation, 15% test\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "    pairs, labels, test_size=0.3, random_state=42, stratify=labels\n",
        ")\n",
        "\n",
        "X_val, X_test, y_val, y_test = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
        ")\n",
        "\n",
        "print(f\"\\nTraining pairs: {len(X_train)}\")\n",
        "print(f\"Validation pairs: {len(X_val)}\")\n",
        "print(f\"Test pairs: {len(X_test)}\")\n",
        "\n",
        "# Create datasets\n",
        "BATCH_SIZE = 32\n",
        "train_dataset = ClausePairDataset(X_train, y_train, preprocessor)\n",
        "val_dataset = ClausePairDataset(X_val, y_val, preprocessor)\n",
        "test_dataset = ClausePairDataset(X_test, y_test, preprocessor)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BiLSTM Model created\n",
            "Total parameters: 3,976,194\n",
            "Training on device: cpu\n",
            "Model parameters: 3,976,194\n",
            "\n",
            "Epoch 1/15\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  33%|███▎      | 73/219 [05:04<10:09,  4.17s/it]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[32m     16\u001b[39m bilstm_trainer = ModelTrainer(bilstm_model)\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m bilstm_history = \u001b[43mbilstm_trainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m15\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\n\u001b[32m     24\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Evaluate\u001b[39;00m\n\u001b[32m     27\u001b[39m bilstm_evaluator = ModelEvaluator(bilstm_model)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Syed Taha Hasan\\Desktop\\i211767_Tahahasan_A2_CS425\\trainer.py:187\u001b[39m, in \u001b[36mModelTrainer.train\u001b[39m\u001b[34m(self, train_loader, val_loader, num_epochs, learning_rate, weight_decay, patience)\u001b[39m\n\u001b[32m    184\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m50\u001b[39m)\n\u001b[32m    186\u001b[39m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m187\u001b[39m train_loss, train_acc = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[38;5;28mself\u001b[39m.train_losses.append(train_loss)\n\u001b[32m    189\u001b[39m \u001b[38;5;28mself\u001b[39m.train_accuracies.append(train_acc)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Syed Taha Hasan\\Desktop\\i211767_Tahahasan_A2_CS425\\trainer.py:101\u001b[39m, in \u001b[36mModelTrainer.train_epoch\u001b[39m\u001b[34m(self, dataloader, criterion, optimizer)\u001b[39m\n\u001b[32m     98\u001b[39m loss = criterion(outputs, labels)\n\u001b[32m    100\u001b[39m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    102\u001b[39m optimizer.step()\n\u001b[32m    104\u001b[39m \u001b[38;5;66;03m# Statistics\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\autograd\\__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\autograd\\graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "# Initialize and train BiLSTM model\n",
        "bilstm_model = BiLSTMSimilarityModel(\n",
        "    vocab_size=preprocessor.vocab_size,\n",
        "    embedding_dim=128,\n",
        "    hidden_dim=256,\n",
        "    num_layers=2,\n",
        "    dropout=0.3,\n",
        "    num_classes=2\n",
        ")\n",
        "\n",
        "print(\"BiLSTM Model created\")\n",
        "total_params = sum(p.numel() for p in bilstm_model.parameters())\n",
        "print(f\"Total parameters: {total_params:,}\")\n",
        "\n",
        "# Train\n",
        "bilstm_trainer = ModelTrainer(bilstm_model)\n",
        "bilstm_history = bilstm_trainer.train(\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    num_epochs=15,\n",
        "    learning_rate=0.001,\n",
        "    weight_decay=1e-5,\n",
        "    patience=5\n",
        ")\n",
        "\n",
        "# Evaluate\n",
        "bilstm_evaluator = ModelEvaluator(bilstm_model)\n",
        "bilstm_metrics = bilstm_evaluator.compute_metrics(test_loader)\n",
        "bilstm_evaluator.print_metrics(bilstm_metrics, \"BiLSTM Model\")\n",
        "bilstm_qualitative = bilstm_evaluator.get_qualitative_results(test_loader, X_test, y_test, num_examples=5)\n",
        "\n",
        "# Plot training history\n",
        "visualizer = TrainingVisualizer()\n",
        "fig = visualizer.plot_training_history(bilstm_history, \"BiLSTM Model\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Model 2: ESIM (Enhanced Sequential Inference Model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize and train ESIM model\n",
        "esim_model = ESIMSimilarityModel(\n",
        "    vocab_size=preprocessor.vocab_size,\n",
        "    embedding_dim=128,\n",
        "    hidden_dim=256,\n",
        "    num_layers=1,\n",
        "    dropout=0.3,\n",
        "    num_classes=2\n",
        ")\n",
        "\n",
        "print(\"ESIM Model created\")\n",
        "print(\"ESIM Architecture:\")\n",
        "print(\"  - Input Encoding: BiLSTM\")\n",
        "print(\"  - Local Inference: Soft attention alignment\")\n",
        "print(\"  - Inference Composition: BiLSTM\")\n",
        "print(\"  - Pooling: Mean and Max\")\n",
        "total_params = sum(p.numel() for p in esim_model.parameters())\n",
        "print(f\"Total parameters: {total_params:,}\")\n",
        "\n",
        "# Train\n",
        "esim_trainer = ModelTrainer(esim_model)\n",
        "esim_history = esim_trainer.train(\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    num_epochs=15,\n",
        "    learning_rate=0.001,\n",
        "    weight_decay=1e-5,\n",
        "    patience=5\n",
        ")\n",
        "\n",
        "# Evaluate\n",
        "esim_evaluator = ModelEvaluator(esim_model)\n",
        "esim_metrics = esim_evaluator.compute_metrics(test_loader)\n",
        "esim_evaluator.print_metrics(esim_metrics, \"ESIM Model\")\n",
        "esim_qualitative = esim_evaluator.get_qualitative_results(test_loader, X_test, y_test, num_examples=5)\n",
        "\n",
        "# Plot training history\n",
        "fig = visualizer.plot_training_history(esim_history, \"ESIM Model\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Model Comparison and Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare models\n",
        "comparison_metrics = [bilstm_metrics, esim_metrics]\n",
        "model_names = [\"BiLSTM\", \"ESIM\"]\n",
        "\n",
        "fig = visualizer.plot_model_comparison(comparison_metrics, model_names)\n",
        "plt.show()\n",
        "\n",
        "# Create comparison table\n",
        "comparison_df = pd.DataFrame({\n",
        "    'Model': model_names,\n",
        "    'Accuracy': [m['accuracy'] for m in comparison_metrics],\n",
        "    'Precision': [m['precision'] for m in comparison_metrics],\n",
        "    'Recall': [m['recall'] for m in comparison_metrics],\n",
        "    'F1-Score': [m['f1_score'] for m in comparison_metrics],\n",
        "    'ROC-AUC': [m['roc_auc'] for m in comparison_metrics],\n",
        "    'PR-AUC': [m['pr_auc'] for m in comparison_metrics],\n",
        "    'Training Time (s)': [bilstm_history['training_time'], esim_history['training_time']]\n",
        "})\n",
        "\n",
        "print(\"\\nModel Comparison:\")\n",
        "print(comparison_df.to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Qualitative Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display qualitative results\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"BiLSTM Model - Sample Correct Predictions\")\n",
        "print(\"=\"*80)\n",
        "for i, example in enumerate(bilstm_qualitative['correct'][:3], 1):\n",
        "    print(f\"\\nExample {i}:\")\n",
        "    print(f\"  Text 1: {example['text1'][:150]}...\")\n",
        "    print(f\"  Text 2: {example['text2'][:150]}...\")\n",
        "    print(f\"  True: {example['true_label']}, Predicted: {example['predicted_label']}, Confidence: {example['confidence']:.3f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ESIM Model - Sample Correct Predictions\")\n",
        "print(\"=\"*80)\n",
        "for i, example in enumerate(esim_qualitative['correct'][:3], 1):\n",
        "    print(f\"\\nExample {i}:\")\n",
        "    print(f\"  Text 1: {example['text1'][:150]}...\")\n",
        "    print(f\"  Text 2: {example['text2'][:150]}...\")\n",
        "    print(f\"  True: {example['true_label']}, Predicted: {example['predicted_label']}, Confidence: {example['confidence']:.3f}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
